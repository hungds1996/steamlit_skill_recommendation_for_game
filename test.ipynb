{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_params\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "from utils import *\n",
    "from skill_recommendation import get_model, RNNModel\n",
    "from constants import passive_skill, all_skill, evolve_requirement\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "active_skill = [s for s in all_skill if s not in passive_skill and s != 'end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "one_step_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_by_value(my_dict):\n",
    "    sorted_tuples = sorted(my_dict.items(), key=lambda item: item[1])\n",
    "    sorted_dict = dict(sorted_tuples)\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Revolver': 2}\n",
      "['end']\n",
      "---------------\n",
      "{'Revolver': 3}\n",
      "['end']\n",
      "---------------\n",
      "{'Revolver': 4}\n",
      "['end']\n",
      "---------------\n",
      "{'Revolver': 5}\n",
      "['end', 'Revolver']\n",
      "---------------\n",
      "{'Guard Badge': 1, 'Revolver': 5}\n",
      "['end', 'Revolver']\n",
      "---------------\n",
      "{'Guard Badge': 2, 'Revolver': 5}\n",
      "['end', 'Revolver']\n",
      "---------------\n",
      "{'Guard Badge': 3, 'Revolver': 5}\n",
      "['end', 'Revolver']\n",
      "---------------\n",
      "{'Guard Badge': 4, 'Revolver': 5}\n",
      "['end', 'Revolver']\n",
      "---------------\n",
      "{'Revolver': 5, 'Guard Badge': 5}\n",
      "['Guard Badge', 'end', 'Revolver']\n",
      "---------------\n",
      "{'Drill': 1, 'Revolver': 5, 'Guard Badge': 5}\n",
      "['Guard Badge', 'end', 'Revolver']\n",
      "---------------\n",
      "['Revolver', 'Revolver', 'Revolver', 'Revolver', 'Revolver', 'Guard Badge', 'Guard Badge', 'Guard Badge', 'Guard Badge', 'Guard Badge', 'Drill']\n",
      "\n",
      "Run time: 0.15007948875427246\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = ['Revolver']\n",
    "result = next_char\n",
    "start = time.time()\n",
    "skills_to_skip = ['end']\n",
    "\n",
    "for n in range(10):\n",
    "    next_char, states, predicted_logits = one_step_model.generate_one_step(next_char, states=states, temperature=0.1)\n",
    "    result+=(next_char)\n",
    "    skills_to_skip = get_skills_to_skip(result)\n",
    "    one_step_model.set_chars_to_skip(skills_to_skip)\n",
    "    \n",
    "    print(sort_dict_by_value(transform_skill_list(result)))\n",
    "    print(skills_to_skip)\n",
    "    print(\"-\"*15)\n",
    "            \n",
    "    \n",
    "end = time.time()\n",
    "print(result)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open(\"./skills\", \"rb\") as fp:\n",
    "#     all_skill = pickle.load(fp)\n",
    "#     print(all_skill)\n",
    "    \n",
    "ids_from_chars = lambda skills: [all_skill.index(s) for s in skills]\n",
    "chars_from_ids = lambda ids: [all_skill[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soundwave Field', 'Guard Badge', 'Revolver', 'Laser Beam', 'Soundwave Field', 'Drill', 'Wanted Poster', 'Drill', 'Drill', 'Revolver', 'Drill', 'Energy Drink', 'Guard Badge', 'Soundwave Field', 'Revolver', 'Revolver', 'Drill', 'Energy Core', 'Wanted Poster', 'Guard Badge', 'Pocket Watch', 'Laser Beam', 'Captain Shield', 'Laser Beam', 'Guard Badge', 'Soundwave Field', 'Soundwave Field', 'Guard Badge', 'Laser Beam', 'Captain Boot', 'Laser Beam', 'Ammo Thruster', 'Ammo Thruster', 'Captain Shield', 'Captain Boot', 'Captain Shield', 'Captain Boot', 'Captain Shield', 'Wanted Poster', 'Captain Boot', 'Energy Core', 'Captain Shield', 'Wanted Poster', 'Wanted Poster', 'Energy Core', 'Energy Core', 'Ammo Thruster', 'Ammo Thruster', 'Ammo Thruster', 'Pocket Watch', 'Captain Boot', 'Energy Core', 'Pocket Watch', 'Pocket Watch', 'Pocket Watch', 'Energy Drink', 'Energy Drink', 'Energy Drink', 'Energy Drink', 'end', 'end', 'end', 'end', 'end', 'end']\n",
      "tf.Tensor(\n",
      "[ 7  9 16 26  7 13 19 13 13 16 13 14  9  7 16 16 13  2 19  9  5 26 10 26\n",
      "  9  7  7  9 26 18 26 22 22 10 18 10 18 10 19 18  2 10 19 19  2  2 22 22\n",
      " 22  5 18  2  5  5  5 14 14 14 14 27 27 27 27 27], shape=(64,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 9 16 26  7 13 19 13 13 16 13 14  9  7 16 16 13  2 19  9  5 26 10 26  9\n",
      "  7  7  9 26 18 26 22 22 10 18 10 18 10 19 18  2 10 19 19  2  2 22 22 22\n",
      "  5 18  2  5  5  5 14 14 14 14 27 27 27 27 27 27], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('sqllab_untitled_query_7_20230719T124351.csv')\n",
    "df_filtered = df[(df['result']=='win')]\n",
    "skills = list(df_filtered['skills'])\n",
    "skills_dataset = []\n",
    "\n",
    "for s in skills:\n",
    "  skills_ind = s.split(',')\n",
    "  skills_ind = [sk.split('_')[-1] for sk in skills_ind if sk.split('_')[-1] != \"\"]\n",
    "\n",
    "  if len(skills_ind) >= 30 and len(skills_ind) <= 65:\n",
    "    skills_dataset.append(skills_ind)\n",
    "#     all_skill += skills_ind\n",
    "# all_skill = list(set(all_skill)) + [\"end\"]\n",
    "\n",
    "ids_from_chars = lambda skills: [all_skill.index(s) for s in skills]\n",
    "chars_from_ids = lambda ids: [all_skill[i] for i in ids]\n",
    "lengths = [len(s) for s in skills_dataset]\n",
    "max_length = max(lengths)\n",
    "dataset_combined = []\n",
    "for skills in skills_dataset:\n",
    "  if len(skills) <= max_length:\n",
    "    to_add = ids_from_chars(skills) + [27] * (max_length - len(skills))\n",
    "    dataset_combined += to_add\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(dataset_combined)\n",
    "sequences = ids_dataset.batch(max_length)\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "  print(input_example)\n",
    "  print(target_example)\n",
    "  \n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "ids_from_chars = lambda skills: [all_skill.index(s) for s in skills]\n",
    "chars_from_ids = lambda ids: [all_skill[i] for i in ids]\n",
    "\n",
    "vocab_size = len(all_skill)\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024\n",
    "\n",
    "model = RNNModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 28) # (batch_size, sequence_length, vocab_size)\n",
      "Prediction shape:  (128, 64, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(3.3318782, shape=(), dtype=float32)\n",
      "Model: \"rnn_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  3584      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3545088   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  28700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,577,372\n",
      "Trainable params: 3,577,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "tf.exp(example_batch_mean_loss).numpy()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 64, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(0.16281609, shape=(), dtype=float32)\n",
      "Model: \"rnn_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  3584      \n",
      "                                                                 \n",
      " gru_2 (GRU)                 multiple                  3545088   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  28700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,577,372\n",
      "Trainable params: 3,577,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_model = RNNModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "new_model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "new_model.built = True\n",
    "new_model.load_weights(\"training_checkpoints/ckpt_150\")\n",
    "\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = new_model(input_example_batch)\n",
    "\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "    tf.exp(example_batch_mean_loss).numpy()\n",
    "    print(new_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1d2cd81c100>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.built = True\n",
    "model.load_weights(\"./training_checkpoints/ckpt_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 28) # (batch_size, sequence_length, vocab_size)\n",
      "Prediction shape:  (128, 64, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(5.106539, shape=(), dtype=float32)\n",
      "Model: \"rnn_model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    multiple                  3584      \n",
      "                                                                 \n",
      " gru_10 (GRU)                multiple                  3545088   \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  28700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,577,372\n",
      "Trainable params: 3,577,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "tf.exp(example_batch_mean_loss).numpy()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 28) # (batch_size, sequence_length, vocab_size)\n",
      "Prediction shape:  (128, 64, 28)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(3.3311408, shape=(), dtype=float32)\n",
      "Model: \"rnn_model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     multiple                  3584      \n",
      "                                                                 \n",
      " gru_8 (GRU)                 multiple                  3545088   \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  28700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,577,372\n",
      "Trainable params: 3,577,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = RNNModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"Mean loss:        \", example_batch_mean_loss)\n",
    "    tf.exp(example_batch_mean_loss).numpy()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\4 Weeks\\python_project\\skill_recommendation\\test.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/4%20Weeks/python_project/skill_recommendation/test.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m new_model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "new_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
